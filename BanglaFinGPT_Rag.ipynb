{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9402127,"sourceType":"datasetVersion","datasetId":5707596},{"sourceId":9809531,"sourceType":"datasetVersion","datasetId":6013244},{"sourceId":9880838,"sourceType":"datasetVersion","datasetId":6066816}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip cache purge\n!pip install accelerate transformers tokenizers\n!pip install einops\n!pip install xformers\n!pip install langchain\n!pip install faiss-gpu\n!pip install sentence_transformers\n!pip install cudf\n!pip install dask-cudf\n!pip install langchain_community\n!pip install --upgrade langchain\n!pip install -U bitsandbytes","metadata":{"_uuid":"68c73827-4d41-46d7-b971-0723a0a15dba","_cell_guid":"7a72dafe-bb70-44a0-af91-5b0d1dfc227e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y torch\n\n","metadata":{"_uuid":"dd1eb750-50dc-4c87-8629-6d4eef3c12e3","_cell_guid":"b18f7a7d-9d1e-4e20-8768-e46414a50974","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip uninstall torch torchvision torchaudio -y\n# !pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n%pip uninstall torchvision -y\n%pip install --pre torchvision --index-url https://download.pytorch.org/whl/nightly/cu121\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n%pip cache purge\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport transformers\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"read_token='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'\nwrite_token=\"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\"","metadata":{"_uuid":"a0e480f0-45ab-4ed7-86c2-d373b3f9ad94","_cell_guid":"cb9b85e4-acf7-44b1-8a1d-9ab17d57c644","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n\n# hf_token = user_secrets.get_secret(\"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\")\nlogin(token = read_token)","metadata":{"_uuid":"caf9ed7f-9ce7-49e0-b60f-06174c2d5267","_cell_guid":"73a3b6bb-7d94-4747-b99f-4a805c8e18dd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())\nprint(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")\ntorch.cuda.empty_cache()","metadata":{"_uuid":"c4a956b5-589c-4d43-9106-636df02da68a","_cell_guid":"dbc602e4-b1e7-44d9-9fd9-11a02a382ddf","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torchvision\n\n\n# print(torch._version_)\n# print(torchvision._version_)","metadata":{"_uuid":"3223c344-5ec7-43c9-a450-35d1eb634f0f","_cell_guid":"f14ad4c9-9e33-419f-9f80-b352755a99fa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport transformers\n\n# model_id = 'meta-llama/Llama-2-7b-chat-hf'\n# model_id = 'Undi95/Meta-Llama-3-8B-hf'\n# model_id = 'meta-llama/Meta-Llama-3-8B'\nmodel_id = 'Zafor158/BanglaLLama-3.2-3b-banglafingpt_finetuned-instruct-v0.0.1'\n# model_id = 'describeai/gemini'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the bitsandbytes library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)\n\n# begin initializing HF items, you need an access token\nhf_auth = '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'\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n    use_auth_token=hf_auth\n)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n#     device_map='auto',\n    device_map={'': 0},\n    use_auth_token=hf_auth\n)\n\n# enable evaluation mode to allow model inference\nmodel.eval()\n\nprint(f\"Model loaded on {device}\")","metadata":{"_uuid":"82ad034e-002c-4d79-b331-7ab6457cd249","_cell_guid":"e0f46f6d-9af9-4772-8b58-242e0b003232","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_auth)\n\n# Define an input prompt\n# input_text = \"ট্যাক্স কি?\"\n\n# Tokenize the input text\n# inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output = model.generate(\n#     inputs[\"input_ids\"],\n#     max_length=500,               # Limit response length to avoid repetition\n#     num_beams=7,                 # Higher beam width for quality\n#     temperature=0.5,             # Lower temperature for more stable output\n#     top_p=0.9,                   # Use nucleus sampling\n#     top_k=40,                    # Limit token selection to top-k choices\n#     no_repeat_ngram_size=3,      # Prevent repeating any 3-gram sequence\n#     repetition_penalty=1.5       # Higher repetition penalty\n# )\n\n# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n# print(f\"Model Response: {generated_text}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, BitsAndBytesConfig","metadata":{"_uuid":"6531a493-fb98-4bdf-bd6d-d1daa833723a","_cell_guid":"e7a8fae2-82cd-4dfb-8841-412a773dc541","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tokenizer = LlamaTokenizer.from_pretrained(model_id)\n# # model = LlamaForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")","metadata":{"_uuid":"ef4a9586-6d55-437a-af09-f9b068e78615","_cell_guid":"6b233a34-0a50-4ead-b9d3-504af75e5880","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" #tokenizer = transformers.AutoTokenizer.from_pretrained(\n  #   model_id,\n   #  use_auth_token=read_token )","metadata":{"_uuid":"9b5d307d-cafb-4584-86f6-48c7299ad227","_cell_guid":"ece25b89-97df-45c0-9221-a8c1eea771d1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stop_list = ['\\nHuman:', '\\n```\\n']\n\nstop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\nstop_token_ids","metadata":{"_uuid":"03bf6f7a-8895-4c0e-9fcb-1e7585904aa4","_cell_guid":"1ab2b541-3cbf-4666-bae5-2ff340620966","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nstop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\nstop_token_ids","metadata":{"_uuid":"93f68709-74cf-4a04-9d94-eccef05222eb","_cell_guid":"6012e712-a953-44e4-8b8d-491d4c66749b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import StoppingCriteria, StoppingCriteriaList\n\n# define custom stopping criteria object\nclass StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        for stop_ids in stop_token_ids:\n            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n                return True\n        return False\n\nstopping_criteria = StoppingCriteriaList([StopOnTokens()])","metadata":{"_uuid":"b2fa7228-ebbf-44d0-a1e6-345edfe07ee1","_cell_guid":"e276cd0b-8170-49fb-ad2c-58102ee5781c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_text = transformers.pipeline(\n    model=model, \n    tokenizer=tokenizer,\n    return_full_text=True,  # langchain expects the full text\n    task='text-generation',\n    # we pass model parameters here too\n    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n    max_new_tokens=512,  # max number of tokens to generate in the output\n    repetition_penalty=1.1  # without this output begins repeating\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_community.llms import HuggingFacePipeline\n\nllm = HuggingFacePipeline(pipeline=generate_text)\n\n# checking again that everything is working fine\nllm(prompt=\"Explain me the difference between Data Lakehouse and Data Warehouse.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom langchain.schema import Document\n\n# Read the Excel file\nfile_path = \"/kaggle/input/bangla-fin-gpt-data-updated/bangla-fin-qa-data.xlsx\"\ndf = pd.read_excel(file_path)\n\n# Convert the DataFrame into a list of Documents with proper formatting\ndocuments = []\n\nfor index, row in df.iterrows():\n    # Extracting specific columns: Question, Answer, Context\n    question = str(row['Question']) if 'Question' in row else ''\n    answer = str(row['Answer']) if 'Answer' in row else ''\n    context = str(row['Context']) if 'Context' in row else ''\n\n    # Formatting the document content\n    # content = f\"Question: {question}\\nAnswer: {answer}\\nContext: {context}\"\n    content = f\"{context}\\n{question}\\n{answer}\\n\"\n\n    \n    # Create a Document object with formatted content\n    document = Document(page_content=content)\n    documents.append(document)\n\nprint(f\"Loaded {len(documents)} formatted documents from the Excel file.\")","metadata":{"_uuid":"ea6f7fd3-bf2b-4b2f-885a-a3f9937a08ea","_cell_guid":"44b529db-0d2a-4e9f-aa20-2635a3c6080d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"documents[0]","metadata":{"_uuid":"c69662e3-ac53-4294-97db-b34633ab141f","_cell_guid":"aef5f420-26a3-49f9-9c16-5a350c043ee6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)","metadata":{"_uuid":"beb84985-dc47-4718-a502-4fa508a9c600","_cell_guid":"2016a505-3abb-4152-babc-5e4601da5b98","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\n\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n# storing embeddings in the vector store\nvectorstore = FAISS.from_documents(all_splits, embeddings)","metadata":{"_uuid":"f2c43d39-29c3-4556-9e4c-86ab43367def","_cell_guid":"f406cbed-4619-46cc-8981-9b429e7c1ea2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def retrieve_documents(query, top_k=5):\n    query_embedding = embeddings.embed_query(query)  # Get embedding for the query\n    results = vectorstore.similarity_search_by_vector(query_embedding, top_k)  # Retrieve top-k documents\n    return results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_answer(query, top_k=5, max_length=500):\n    # Retrieve relevant documents from FAISS\n    retrieved_docs = retrieve_documents(query, top_k)\n\n    # Combine the retrieved documents into a context string for the model\n    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n\n    # Tokenize the query and context to feed into the model\n    input_text = f\"{context}\\nQuery: {query}\\nAnswer:\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Generate the answer using the fine-tuned model\n# output = model.generate(\n#     inputs[\"input_ids\"],\n#     max_length=2000,\n#     num_beams=7,\n#     temperature=0.5,\n#     top_p=0.9,\n#     top_k=40,\n#     no_repeat_ngram_size=3,\n#     repetition_penalty=1.5,\n#     stopping_criteria=stopping_criteria  # Custom stopping criteria defined earlier\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ndef generate_response(input_text):\n    # Tokenize the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n\n    # Generate the answer using the fine-tuned model\n    output = model.generate(\n        inputs[\"input_ids\"],\n        max_length=2000,\n        num_beams=7,\n        temperature=0.5,\n        top_p=0.9,\n        top_k=40,\n        no_repeat_ngram_size=3,\n        repetition_penalty=1.5,\n        stopping_criteria=stopping_criteria  # Custom stopping criteria defined earlier\n    )\n\n    # Decode the output to get the generated response\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    return generated_text  # Return the generated text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_answer(query, top_k=5, max_length=500):\n    # Retrieve relevant documents from FAISS\n    retrieved_docs = retrieve_documents(query, top_k)\n\n    # Combine the retrieved documents into a context string for the model\n    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n\n    # Tokenize the query and context to feed into the model\n    input_text = f\"{context}\\nQuery: {query}\\nAnswer:\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n\n    # Generate the answer using the fine-tuned model\n    output = model.generate(\n        inputs[\"input_ids\"],\n        max_length=1000,\n        num_beams=7,\n        temperature=0.5,\n        top_p=0.9,\n        top_k=40,\n        no_repeat_ngram_size=3,\n        repetition_penalty=1.5,\n    )\n\n    # Decode the output to get the generated response\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    return generated_text  # Return the generated text\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of how to call the function\ninput_text = \"কীভাবে ইলেকট্রনিক রেকর্ড সংরক্ষণ করা হয়?\"\nresponse = generate_answer(input_text)\nprint(f\"Model Response: {response}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import re\n\n# def extract_content(text, section):\n#     \"\"\"\n#     Extracts the content of the specified section ('Helpful Answer' or 'Answer') from the given text.\n    \n#     Parameters:\n#         text (str): The input text containing the sections.\n#         section (str): The section to extract ('Helpful Answer' or 'Answer').\n        \n#     Returns:\n#         str: The content of the specified section or an empty string if the section is not found.\n#     \"\"\"\n#     # Define the pattern to match the section and capture the content after it\n#     pattern = rf\"{section}:\\s*(.+)\"\n    \n#     # Use regular expression to search for the section content\n#     match = re.search(pattern, text, re.IGNORECASE)\n    \n#     # Return the content if found, otherwise return an empty string\n#     return match.group(1).strip() if match else \"\"\n\n# # Example usage","metadata":{"_uuid":"07899d6c-0d7b-429e-bb0b-7eb2e8421483","_cell_guid":"1111103c-ff23-4f43-91f5-951e03c5c087","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.chains import ConversationalRetrievalChain\n\nchain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)","metadata":{"_uuid":"b3aba102-281c-4a61-9ba5-54c436014005","_cell_guid":"39846d93-12bd-43bc-8b20-5a7cca38a0c0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# chat_history = []\n\n# query = \"\"\"\n# Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# Keep the response in details and precise. Don't repeat same words or lines repetadely. The response should have to be meaningful. The content under Input is the main Question. You should not repeat anythings from the prompt or instruction that I give you. Follow the instruction and input but don't repeat the words written in it. You should answer the question In Bangla. The response have to be in Bangla.\n\n# ### Input:\n# What is the process for filing income tax returns for individual taxpayers?\n\n# \"\"\"\n# #result = chain({\"question\": query, \"chat_history\": chat_history})\n# result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n\n\n# # print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"1665bb7e-65be-4ab8-893b-bf25e0f9aabe","_cell_guid":"f89f690b-b574-4d0b-98a2-7a1365251476","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# Keep the response in details and precise. Don't repeat same words or lines repetadely. The response should have to be meaningful. The content under Input is the main Question. You should not repeat anythings from the prompt or instruction that I give you. Follow the instruction and input but don't repeat the words written in it. You should answer the question In Bangla. The response have to be in Bangla.\n\n# ### Input:\n# হাইকোর্ট বিভাগে শুনানী গ্রহণের জন্য বিচারকগণ কে কে??\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# result['answer']","metadata":{"_uuid":"1b34c2b6-00a4-48ba-b778-45b14c49cd00","_cell_guid":"79ce3627-d457-4b0d-a1c6-7b1385a5dee8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# Keep the response in details and precise. Don't repeat same words or lines repetadely. The response should have to be meaningful. The content under Input is the main Question. You should not repeat anythings from the prompt or instruction that I give you. Follow the instruction and input but don't repeat the words written in it. You should answer the question In Bangla. The response have to be in Bangla. Don't use English Language for response. Use Bangla Language for response.\n\n# ### Input:\n# In what activities can customs officers be engaged?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"90e25555-f8c0-4cd7-bb79-e92ef915b36c","_cell_guid":"9041c6e1-6dfe-46f6-aa75-969fb9254207","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# Keep the response in details and precise. Don't repeat same words or lines repetadely. The response should have to be meaningful. The content under Input is the main Question. You should not repeat anythings from the prompt or instruction that I give you. Follow the instruction and input but don't repeat the words written in it. You should answer the question In Bangla. The response have to be in Bangla. Don't use English Language for response. Use Bangla Language for response.\n\n# ### Input:\n# হাইকোর্ট বিভাগে শুনানী গ্রহণের জন্য বিচারকগণ কে কে?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"d781b947-0017-42b3-b64c-b12ca51fe6f2","_cell_guid":"d9c93c7a-ade8-48f0-9cf0-f7b0b1b768bd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n# ### Input:\n# ব্যক্তিগত করদাতাদের জন্য আয়কর রিটার্ন দাখিল করার প্রক্রিয়া কি?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"f3bdd7b3-aa60-4be8-a78b-71a5b4819c0c","_cell_guid":"b987b87e-f7e0-4ed4-972b-1af7f4833455","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n# ### Input:\n# কিভাবে কোনো কাস্টমস কর্মকর্তা অন্য কোনো সরকারি কর্মকর্তার উপর অর্পণ করিতে পারে?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"1b718302-120f-4534-b713-c196592e5d25","_cell_guid":"7cbd7d4f-b1ba-4066-81e0-5fa221634275","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n# ### Input:\n# কাস্টমস কর্মকর্তা হিসেবে কাজ করতে আগ্রহী হতে হলে কি যোগ্যতা ও অভিজ্ঞতা প্রয়োজন?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"e5d82a71-ebb5-417f-bc52-6f1f61e08373","_cell_guid":"69e65aaa-14bb-4450-85ce-681b66a27091","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n# ### Input:\n# ট্র্যানজ্যাকশন বেইজড অডিটের প্রথম ধাপে কী করতে হবে?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))","metadata":{"_uuid":"052dc722-54a5-4dde-af4a-4509a0d380a1","_cell_guid":"e9f755d7-9e43-413d-b124-8a94f040682c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n# ### Input:\n# ট্র্যানজ্যাকশন বেইজড অডিটের প্রথম ধাপে কী করতে হবে?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))","metadata":{"_uuid":"f75f5adf-2581-420b-a2a8-0ff0d3cdefd0","_cell_guid":"f21c0e66-988c-415f-951b-5bb86eabb938","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q streamlit","metadata":{"_uuid":"0cb34dc0-ba5d-41bd-9176-6698bd6dabff","_cell_guid":"93f2adaf-5685-4b48-89dc-1440c6ae58ac","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Your chain function and extract_content should already be defined\n# For example:\n# from your_chain_module import chain, extract_content\n\ndef generate_prompt(user_question):\n    query_template = \"\"\"\n    Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n    ### Instruction:\n    For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n    ### Input:\n    {}\n    \"\"\"\n    # Replace {} with the dynamic user question\n    return query_template.format(user_question)","metadata":{"_uuid":"1c6d28b2-26e8-4894-965a-684a04194b6f","_cell_guid":"fd102196-5f4e-4177-a336-e517bca2b1f6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --no-dependencies --quiet streamlit","metadata":{"_uuid":"3a712ad3-2ceb-43d9-b878-b4b0b16a3b59","_cell_guid":"b4a414a5-c2bc-4875-9454-14667cbd4044","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%writefile app.py\n# import streamlit as st\n# from huggingface_hub import login\n# from kaggle_secrets import UserSecretsClient\n# import re\n# import torch\n# from torch import cuda, bfloat16\n# import transformers\n# from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, BitsAndBytesConfig\n# from langchain_community.llms import HuggingFacePipeline\n# import pandas as pd\n# from langchain.schema import Document\n\n# def extract_content(text, section):\n#     \"\"\"\n#     Extracts the content of the specified section ('Helpful Answer' or 'Answer') from the given text.\n    \n#     Parameters:\n#         text (str): The input text containing the sections.\n#         section (str): The section to extract ('Helpful Answer' or 'Answer').\n        \n#     Returns:\n#         str: The content of the specified section or an empty string if the section is not found.\n#     \"\"\"\n#     # Define the pattern to match the section and capture the content after it\n#     pattern = rf\"{section}:\\s*(.+)\"\n    \n#     # Use regular expression to search for the section content\n#     match = re.search(pattern, text, re.IGNORECASE)\n    \n#     # Return the content if found, otherwise return an empty string\n#     return match.group(1).strip() if match else \"\"\n\n# # Example usage\n\n# from transformers import StoppingCriteria, StoppingCriteriaList\n\n# # define custom stopping criteria object\n# class StopOnTokens(StoppingCriteria):\n#     def _call_(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n#         for stop_ids in stop_token_ids:\n#             if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n#                 return True\n#         return False\n\n# def generate_prompt(user_question):\n#     query_template = \"\"\"\n#     Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n#     ### Instruction:\n#     For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n#     ### Input:\n#     {}\n#     \"\"\"\n    \n# def main():\n#     read_token='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'\n    \n \n#     user_secrets = UserSecretsClient()\n\n#     # hf_token = user_secrets.get_secret(\"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\")\n#     login(token = read_token)\n    \n  \n#     print(torch.cuda.is_available())\n#     print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")\n#     torch.cuda.empty_cache()\n    \n    \n\n#     # model_id = 'meta-llama/Llama-2-7b-chat-hf'\n#     # model_id = 'Undi95/Meta-Llama-3-8B-hf'\n#     # model_id = 'meta-llama/Meta-Llama-3-8B'\n#     model_id = 'OdiaGenAI/odiagenAI-bengali-base-model-v1'\n#     # model_id = 'describeai/gemini'\n\n#     device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n#     # set quantization configuration to load large model with less GPU memory\n#     # this requires the bitsandbytes library\n#     bnb_config = transformers.BitsAndBytesConfig(\n#         load_in_4bit=True,\n#         bnb_4bit_quant_type='nf4',\n#         bnb_4bit_use_double_quant=True,\n#         bnb_4bit_compute_dtype=bfloat16\n#     )\n\n#     # begin initializing HF items, you need an access token\n#     hf_auth = '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'\n#     model_config = transformers.AutoConfig.from_pretrained(\n#         model_id,\n#         use_auth_token=hf_auth\n#     )\n\n#     model = transformers.AutoModelForCausalLM.from_pretrained(\n#         model_id,\n#         trust_remote_code=True,\n#         config=model_config,\n#         quantization_config=bnb_config,\n#     #     device_map='auto',\n#         device_map={'': 0},\n#         use_auth_token=hf_auth\n#     )\n\n#     # enable evaluation mode to allow model inference\n#     model.eval()\n    \n    \n#     tokenizer = LlamaTokenizer.from_pretrained(model_id)\n\n#     stop_list = ['\\nHuman:', '\\n```\\n']\n\n#     stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n\n#     stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n    \n#     stop_token_ids = [x.cpu().tolist() for x in stop_token_ids]  # Convert to CPU and then to list\n\n    \n    \n\n#     stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n    \n#     generate_text = transformers.pipeline(\n#         model=model, \n#         tokenizer=tokenizer,\n#         return_full_text=True,  # langchain expects the full text\n#         task='text-generation',\n#         # we pass model parameters here too\n#         stopping_criteria=stopping_criteria,  # without this model rambles during chat\n#         temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n#         max_new_tokens=512,  # max number of tokens to generate in the output\n#         repetition_penalty=1.1  # without this output begins repeating\n#     )\n    \n    \n\n#     llm = HuggingFacePipeline(pipeline=generate_text)\n    \n    \n\n#     # Read the Excel file\n#     file_path = \"/kaggle/input/bangla-fin-gpt-data-updated/bangla-fin-qa-data.xlsx\"\n#     df = pd.read_excel(file_path)\n\n#     # Convert the DataFrame into a list of Documents with proper formatting\n#     documents = []\n\n#     for index, row in df.iterrows():\n#         # Extracting specific columns: Question, Answer, Context\n#         question = str(row['Question']) if 'Question' in row else ''\n#         answer = str(row['Answer']) if 'Answer' in row else ''\n#         context = str(row['Context']) if 'Context' in row else ''\n\n#         # Formatting the document content\n#         # content = f\"Question: {question}\\nAnswer: {answer}\\nContext: {context}\"\n#         content = f\"{context}\\n{question}\\n{answer}\\n\"\n\n\n#         # Create a Document object with formatted content\n#         document = Document(page_content=content)\n#         documents.append(document)\n\n#     from langchain.text_splitter import RecursiveCharacterTextSplitter\n\n#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n#     all_splits = text_splitter.split_documents(documents)\n    \n    \n#     from langchain.embeddings import HuggingFaceEmbeddings\n#     from langchain.vectorstores import FAISS\n\n#     model_name = \"sentence-transformers/all-mpnet-base-v2\"\n#     model_kwargs = {\"device\": \"cuda\"}\n\n#     embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n#     # storing embeddings in the vector store\n#     vectorstore = FAISS.from_documents(all_splits, embeddings)\n    \n#     from langchain.chains import ConversationalRetrievalChain\n\n#     chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)\n    \n#     st.title(\"Bangla Chatbot\")\n#     st.write(\"This chatbot generates meaningful responses in Bangla.\")\n\n#     # Initialize the chat history\n#     if \"messages\" not in st.session_state:\n#         st.session_state.messages = []\n\n#     # Display chat messgaes from history on app rerun\n#     for message in st.session_state.messages:\n#         with st.chat_message(message['role']):\n#             st.markdown(message['content'])\n\n   \n#     # Replace {} with the dynamic user question\n#     return query_template.format(user_question)\n\n#     # React to user input\n#     if prompt := st.chat_input(\"What is up?\"):\n#             # Display assistant response in chat message container\n#             with st.chat_message(\"user\"):\n#                 st.markdown(prompt)\n\n#             # Add user message to chat history\n#             st.session_state.messages.append({\"role\": \"question\", \"content\": prompt})\n\n#             result = chain({\"question\": generate_prompt(prompt), \"chat_history\": []})\n\n#             result_to_show = extract_content(result['answer'], 'Helpful Answer')\n#             # Display assistant response in chat message container\n#             with st.chat_message(\"assistant\"):\n#                 st.markdown(result_to_show);\n\n#             # Add assistant response to chat history\n#             st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n\n        \n# if __name__ == '__main__':\n#     main()","metadata":{"_uuid":"ce855be5-7018-4356-acfd-d4c6761f7f1d","_cell_guid":"3d14e20d-88e4-4f47-ba28-94e62068f05e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile app.py\nimport streamlit as st\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport re\nimport torch\nfrom torch import cuda, bfloat16\nimport transformers\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, BitsAndBytesConfig\nfrom langchain_community.llms import HuggingFacePipeline\nimport pandas as pd\nfrom langchain.schema import Document\n\n# Function to extract content\ndef extract_content(text, section):\n    pattern = rf\"{section}:\\s*(.+)\"\n    match = re.search(pattern, text, re.IGNORECASE)\n    return match.group(1).strip() if match else \"\"\n\n# Stopping criteria\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\n\ndef generate_prompt(user_question):\n    query_template = \"\"\"\n    Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n    ### Instruction:\n    For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n    ### Input:\n    {}\n    \"\"\"\n    return query_template.format(user_question)\n\n# Initialization of models, vectorstore, and chain (runs only once)\n@st.cache_resource(show_spinner=False)\ndef initialize_model_and_chain():\n    read_token = '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'\n    user_secrets = UserSecretsClient()\n    login(token=read_token)\n    torch.cuda.empty_cache()\n\n    # Model settings\n    model_id = 'OdiaGenAI/odiagenAI-bengali-base-model-v1'\n    device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n    # Load the model with quantization\n    bnb_config = transformers.BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=bfloat16\n    )\n    \n    hf_auth = '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'\n    model_config = transformers.AutoConfig.from_pretrained(\n        model_id,\n        use_auth_token=hf_auth\n    )\n    \n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_id,\n        trust_remote_code=True,\n        config=model_config,\n        quantization_config=bnb_config,\n        device_map={'': 0},\n        use_auth_token=hf_auth\n    )\n\n    tokenizer = LlamaTokenizer.from_pretrained(model_id)\n\n    # Stopping tokens\n    stop_list = ['\\nHuman:', '\\n```\\n']\n    stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n    stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n    stop_token_ids = [x.cpu().tolist() for x in stop_token_ids]\n\n#     class StopOnTokens(StoppingCriteria):\n#         def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n#             for stop_ids in stop_token_ids:\n#                 if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n#                     return True\n#             return False\n    class StopOnTokens(StoppingCriteria):\n        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n            # Loop through the stop token IDs and check if they appear in the current input_ids\n            for stop_ids in stop_token_ids:\n                stop_ids_tensor = torch.tensor(stop_ids, device=input_ids.device)  # Convert list to tensor\n                if torch.eq(input_ids[0][-len(stop_ids):], stop_ids_tensor).all():\n                    return True\n            return False\n\n    \n    stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n\n    # Text generation pipeline\n    generate_text = transformers.pipeline(\n        model=model, \n        tokenizer=tokenizer,\n        return_full_text=True,\n        task='text-generation',\n        stopping_criteria=stopping_criteria,\n        temperature=0.1,\n        max_new_tokens=512,\n        repetition_penalty=1.1\n    )\n\n    llm = HuggingFacePipeline(pipeline=generate_text)\n\n    # Loading and preparing the data\n    file_path = \"/kaggle/input/finance/BanglaFinGpt_updated.xlsx\"\n    df = pd.read_excel(file_path)\n\n    # Convert the DataFrame into a list of Documents\n    documents = []\n    for index, row in df.iterrows():\n        question = str(row['Question']) if 'Question' in row else ''\n        answer = str(row['Answer']) if 'Answer' in row else ''\n        context = str(row['Context']) if 'Context' in row else ''\n        content = f\"{context}\\n{question}\\n{answer}\\n\"\n        document = Document(page_content=content)\n        documents.append(document)\n\n    # Splitting documents\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n    all_splits = text_splitter.split_documents(documents)\n\n    # Embeddings and FAISS\n    from langchain.embeddings import HuggingFaceEmbeddings\n    from langchain.vectorstores import FAISS\n    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n    model_kwargs = {\"device\": \"cuda\"}\n    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n    # Store embeddings in the vectorstore\n    vectorstore = FAISS.from_documents(all_splits, embeddings)\n\n    # Create conversational retrieval chain (only once)\n    from langchain.chains import ConversationalRetrievalChain\n    chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)\n\n    return chain\n\n# Main function\ndef main():\n    st.title(\"Bangla Chatbot\")\n    st.write(\"This chatbot generates meaningful responses in Bangla.\")\n\n    # Initialize chain (only runs once)\n    chain = initialize_model_and_chain()\n\n    # Initialize the chat history\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    # Display chat messages from history on app rerun\n    for message in st.session_state.messages:\n        with st.chat_message(message['role']):\n            st.markdown(message['content'])\n\n    # React to user input\n    if prompt := st.chat_input(\"What is up?\"):\n        # Display user message in chat message container\n        with st.chat_message(\"user\"):\n            st.markdown(prompt)\n\n        # Add user message to chat history\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\n        # Conversational chain logic (no need to create chain every time)\n        result = chain({\"question\": generate_prompt(prompt), \"chat_history\": []})\n\n        # Extract response\n        result_to_show = extract_content(result['answer'], 'Helpful Answer')\n\n        # Display assistant response in chat message container\n        with st.chat_message(\"assistant\"):\n            st.markdown(result_to_show)\n\n        # Add assistant response to chat history\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": result_to_show})\n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"36f622ac-b111-41fd-93ca-5452750f857f","_cell_guid":"af1b9e19-fcf4-4b8c-af6e-ef0c6605d10a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc \\\n  | tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" \\\n  | tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && apt install ngrok","metadata":{"_uuid":"770ce467-d8b0-444e-8df5-3a140ef14446","_cell_guid":"628d3808-2845-4043-951c-f77c0f98c370","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --quiet pyngrok\n!pip install --no-dependencies --quiet protobuf==3.20.*   #==4.21.12!pip install --no-dependencies --quiet validators\n!pip install --no-dependencies --quiet validators","metadata":{"_uuid":"104898ae-5bee-4126-83a9-0ea46f1f162f","_cell_guid":"6589cb94-4ec5-44e8-8500-97c75d7c8273","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ngrok config add-authtoken 2m3kux5R5Tqcfm2NgpjeWiADTKC_6JyUqUUyA1B6F1nUE5MQg","metadata":{"_uuid":"a9ac6898-83c1-4c81-8464-1593efad9a23","_cell_guid":"937c2090-8ecd-48b1-b3ff-f6d2bc686f01","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !streamlit run --server.port 80 my_app.py > /dev/null\n\nimport subprocess\nsubprocess.Popen([\"streamlit\", \"run\", \"--server.port\", \"80\", \"app.py\"])","metadata":{"_uuid":"5c545bb5-2132-489b-9adc-2aa62d80844c","_cell_guid":"159b1d89-deea-4c13-9e1e-3ec9c609a57f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install watchdog","metadata":{"_uuid":"ca73ee4f-d583-4e7a-99e4-d768f99e876b","_cell_guid":"264fbca0-7224-4ce6-9aab-5f1729570b8a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ngrok http 80","metadata":{"_uuid":"999bcc76-704b-4226-92a3-18d02e09180c","_cell_guid":"18b4b699-2584-4f4c-b51a-497eeafd600c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"641cb1f8-f4e8-41ea-a8ea-6e71575ff6d0","_cell_guid":"fa9f07c4-3cd9-43e7-aa29-239b85d47add","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"c6b49fb1-e96c-4f84-8189-418f99fea340","_cell_guid":"cad1bdaa-281a-4f0d-8313-c8d96ae29960","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!streamlit run app.py &>/content/logs.txt & curl ipv4.icanhazip.com","metadata":{"_uuid":"70730324-1781-4342-9b2d-0589b7a47635","_cell_guid":"9aafac5e-393e-4b42-86ea-0fe4f985e0ab","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    'import sys, json; print(\"Execute the next cell and the go to the following URL: \" +json.load(sys.stdin)[\"tunnels\"][0][\"public_url\"])'","metadata":{"_uuid":"184d2649-ef93-4dda-8290-066038434369","_cell_guid":"146afb03-a05e-441f-aa22-c008eb380375","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# run your file containing the script \n\n# your uploaded file\n!streamlit run ./eda_basketball.py\n\n# or\n\n# newly created file\n# !streamlit run ./my_app.py","metadata":{"_uuid":"33d4428d-ba8c-41dd-9110-33663cf1de30","_cell_guid":"ad120a96-6d01-4df3-9dbe-1708151ee8e1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !npx localtunnel --port 8501\n# !npx localtunnel --port 8501 --subdomain mycustomsubdomain","metadata":{"_uuid":"6d701c17-77e9-47e8-9ac3-6ede03aa8935","_cell_guid":"1750195e-15e8-41a2-abb5-c8ae2ffa9e28","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"b4791b04-d3cd-46a5-a2fb-be3b8f126b08","_cell_guid":"b71c1252-e33e-4eed-b760-82d7baf0f5c1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" [code] {\"jupyter\":{\"outputs_hidden\":false}}","metadata":{"_uuid":"1903690e-cc48-4f32-a0d9-143b5dbfc979","_cell_guid":"df787bd0-9aac-44fa-afb9-0c06b13f6165","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null}]}